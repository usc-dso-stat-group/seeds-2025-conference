<!DOCTYPE html>
<html lang='en'>

<head>
    <base href="..">
    <link rel="shortcut icon" type="image/png" href="assets/favicon.png"/>
    <link rel="stylesheet" type="text/css" media="all" href="assets/main.css"/>
    <meta name="description" content="SEEDS Conference">
    <meta name="resource-type" content="document">
    <meta name="distribution" content="global">
    <meta name="KeyWords" content="Conference">
    <title>Sessions | SEEDS Conference</title>
</head>

<body>

    <div class="banner">
        <img src="assets/banner.jpg" alt="SEEDS Conference">
        <div class="top-left">
            <span class="title1">SEEDS</span><span class="title2">Conference</span> <span class="year">2025</span>
        </div>
        <div class="bottom-right">
            January 8-11, 2025 <br> University of Southern California, Los Angeles (CA)
        </div>
    </div>

    <style>
    .navigation {
        display: flex;
        justify-content: space-around;
        padding: 10px;
        background-color: #f8f9fa;
        border: 1px solid #ddd;
        border-radius: 5px;
    }
    .navigation a {
        text-decoration: none;
        padding: 10px 15px;
        color: #007bff;
        font-size: 24px; /* Increased font size */
        font-weight: 500; /* Medium weight for balance */
        border: 1px solid transparent;
        border-radius: 4px;
        transition: background-color 0.3s ease, color 0.3s ease;
    }
    .navigation a:hover {
        background-color: #007bff;
        color: white;
        border-color: #0056b3;
    }
    .navigation a.current {
        font-weight: bold;
        color: white;
        background-color: #007bff;
    }
</style>

<div class="navigation">
    <a title="Conference Home Page" href=".">Home</a>
    <a title="Register for the Conference" href="registration">Registration</a>
    <a title="Explore Short Courses" href="short courses">Short Courses</a>
    <a class="current" title="View Conference Sessions" href="sessions">Sessions</a>
    <a title="See the Full Schedule" href="schedule">Schedule</a>
    <a title="Find Venue Details" href="venue">Venue</a>
</div>

    <br>

    <a name=“keynote_presentations”>
    <h3>Keynote Presentations</h4>
    </a>

    <table>
        <tr>
            <td class="date" rowspan="2">
            </td>
            <td class="title">
                <a href="https://fan.princeton.edu/">Jianqing Fan (Princeton University)</a>
            </td>
        </tr>
        <tr>
            <td class="speaker">
             Spectral Ranking Inferences Based on General Multiway Comparisons
            </td>
        </tr>
            <td class="date" rowspan="2">

            </td>
        <tr>
            <td class="abstract">
              Abstract: This paper studies the performance of the spectral method in the estimation and uncertainty quantification of the unobserved preference scores of compared entities in a general and more realistic setup. Specifically, the comparison graph consists of hyper-edges of possible heterogeneous sizes, and the number of comparisons can be as low as one for a given hyper-edge. Such a setting is pervasive in real applications, circumventing the need to specify the graph randomness and the restrictive homogeneous sampling assumption imposed in the commonly used Bradley-Terry-Luce (BTL) or Plackett-Luce (PL) models. Furthermore, in scenarios where the BTL or PL models are appropriate, we unravel the relationship between the spectral estimator and the Maximum Likelihood Estimator (MLE). We discover that a two-step spectral method, where we apply the optimal weighting estimated from the equal weighting vanilla spectral method, can achieve the same asymptotic efficiency as the MLE. Given the asymptotic distributions of the estimated preference scores, we also introduce a comprehensive framework to carry out both one-sample and two-sample ranking inferences, applicable to both fixed and random graph settings. It is noteworthy that this is the first time effective two-sample rank testing methods have been proposed. Finally, we substantiate our findings via comprehensive numerical simulations and subsequently apply our developed methodologies to perform statistical inferences for statistical journals and movie rankings. (Joint work with Zhipeng Lou,  Weichen Wang, and Mengxin Yu)
            </td>
        </tr>
    </table>

    <table>
        <tr>
            <td class="date" rowspan="2">
            </td>
            <td class="title">
                <a href="https://www.hsph.harvard.edu/profile/xihong-lin/">Xihong Lin (Harvard University)</a>
            </td>
        </tr>
        <tr>
            <td class="speaker">
              Navigate the Crossroad of Statistics, ML/AI and Domain Science
            </td>
        </tr>
            <td class="date" rowspan="2">

            </td>
        <tr>
            <td class="abstract">
Abstract: The data science ecosystem encompasses data fairness, scalable statistical and ML/AI methods and tools, interpretable data analysis, and trustworthy decision-making. Rapid advancements in AI have revolutionized data utilization and enabled machines to learn from data more effectively. Statistics, as the science of learning from data while accounting for uncertainty, plays a pivotal role in addressing complex real-world problems and facilitating trustworthy decision-making. In this talk, I will discuss the challenges and opportunities as we navigate the crossroad of statistics and AI, including how to build an end-to-end scalable data science ecosystem, leverage AI/ML-prediction to empower statistical analysis of biobank data,  transfer inference for genetic association analysis,  build the whole genome variant functional annotation database and portal FAVOR and FAVOR-GPT,  and incorporate multi-faceted variant functional annotation to boost power of whole genome sequencing end-to-end analysis. This talk aims to ignite proactive and thought-provoking discussions, foster cross-disciplinary collaboration, and cultivate open-minded approaches to advance scientific discovery.
            </td>
        </tr>
    </table>

    <table>
        <tr>
            <td class="date" rowspan="2">
            </td>
            <td class="title">
                <a href="https://www.statslab.cam.ac.uk/~rjs57/">Richard J. Samworth (University of Cambridge)</a>
            </td>
        </tr>
        <tr>
            <td class="speaker">
            How should we do linear regression?
            </td>
        </tr>
            <td class="date" rowspan="2">

            </td>
        <tr>
            <td class="abstract">
                Abstract: In the context of linear regression, we construct a data-driven convex loss function with respect to which empirical risk minimisation yields optimal asymptotic variance in the downstream estimation of the regression coefficients. Our semiparametric approach targets the best decreasing approximation of the derivative of the log-density of the noise distribution. At the population level, this fitting process is a nonparametric extension of score matching, corresponding to a log-concave projection of the noise distribution with respect to the Fisher divergence. The procedure is computationally efficient, and we prove that our procedure attains the minimal asymptotic covariance among all convex M-estimators. As an example of a non-log-concave setting, for Cauchy errors, the optimal convex loss function is Huber-like, and our procedure yields an asymptotic efficiency greater than 0.87 relative to the oracle maximum likelihood estimator of the regression coefficients that uses knowledge of this error distribution; in this sense, we obtain robustness without sacrificing much efficiency. 
            </td>
        </tr>
    </table>

    <h2>Invited Talks</h2>
    <p>
     <table>
        <tr>
            <td class="date" rowspan="2">
            </td>
            <td class="title">
                <a href="https://thomasberrett.github.io/">Thomas Berrett (University of Warwick)</a>
            </td>
        </tr>
        <tr>
            <td class="speaker">
             Efficient estimation with incomplete data via generalised ANOVA decompositions
            </td>
        </tr>
            <td class="date" rowspan="2">

            </td>
        <tr>
            <td class="abstract">
              Abstract: In this talk I will present recent work (https://arxiv.org/abs/2409.05729) on efficient estimation with incomplete data, covering problems arising in semi-supervised learning, data fusion and missing data literatures. Our task is to estimate simple mean functionals given access to a complete dataset that is supplemented by additional incomplete datasets. In particular, we aim to use the incomplete data to reduce the variance of the naive complete-case estimator, and to characterise the minimal asymptotic risk among all estimators. Results of this type exist for monotonic missingness structures, such as those arising in semi-supervised learning and longitudinal studies, but in this work we consider more general settings. We show that the optimal variance can be expressed through the minimal value of a quadratic optimisation problem over a function space, thus establishing a fundamental link between these estimation problems and the theory of generalised ANOVA decompositions. We introduce an estimator that is proved to attain this minimal risk and to be approximately normally distributed, and use this to construct confidence intervals.
        </td>
        </tr>
    </table>
      <table>
        <tr>
            <td class="date" rowspan="2">
            </td>
            <td class="title">
                <a href="https://www.gaolan.page/">Lan Gao (The University of Tennessee Knoxville)</a>
            </td>
        </tr>
        <tr>
            <td class="speaker">
             ARK: Robust Knockoffs Inference with Coupling
            </td>
        </tr>
            <td class="date" rowspan="2">

            </td>
        <tr>
            <td class="abstract">
              Abstract: We investigate the robustness of the model-X knockoffs framework with respect to the misspecified or estimated feature distribution. We achieve such a goal by theoretically studying the feature selection performance of a practically implemented knockoffs algorithm, which we name as the approximate knockoffs (ARK) procedure, under the measures of the false discovery rate (FDR) and family wise error rate (FWER). The approximate knockoffs procedure differs from the model-X knockoffs procedure only in that the former uses the misspecified or estimated feature distribution. A key technique in our theoretical analyses is to couple the approximate knockoffs procedure with the model-X knockoffs procedure so that random variables in these two procedures can be close in realizations. We prove that if such coupled model-X knockoffs procedure exists, the approximate knockoffs procedure can achieve the asymptotic FDR or FWER control at the target level. We showcase three specific constructions of such coupled model-X knockoff variables, verifying their existence and justifying the robustness of the model-X knockoffs framework.
            </td>
        </tr>
    </table>

    <table>
        <tr>
            <td class="date" rowspan="2">
            </td>
            <td class="title">
                <a href="https://www.gaolan.page/">Annie Qu (UC Irvine)</a>
            </td>
        </tr>
        <tr>
            <td class="speaker">
             Stage-Aware Learning for Dynamic Treatments
            </td>
        </tr>
            <td class="date" rowspan="2">

            </td>
        <tr>
            <td class="abstract">
              Abstract: Recent advances in dynamic treatment regimes (DTRs) provide powerful optimal treatment searching algorithms, which are tailored to individuals’ specific needs and able to maximize their expected clinical benefits. However, existing algorithms could suffer from insufficient sample size under optimal treatments, especially for chronic diseases involving long stages of decision-making. To address these challenges, we propose a novel individualized learning method which estimates the DTR with a focus on prioritizing alignment between the observed treatment trajectory and the one obtained by the optimal regime across decision stages. By relaxing the restriction that the observed trajectory must be fully aligned with the optimal treatments, our approach substantially improves the sample efficiency and stability of inverse probability weighted based methods. In particular, the proposed learning scheme builds a more general framework which includes the popular outcome weighted learning framework as a special case of ours. Moreover, we introduce the notion of stage importance scores along with an attention mechanism to explicitly account for heterogeneity among decision stages. We establish the theoretical properties of the proposed approach, including the Fisher consistency and finite-sample performance bound. Empirically, we evaluate the proposed method in extensive simulated environments and a real case study for COVID-19 pandemic.
        </td>
        </tr>
    </table>


    <table>
        <tr>
            <td class="date" rowspan="2">
            </td>
            <td class="title">
                <a href="https://warwick.ac.uk/fac/sci/statistics/staff/academic-research/yu/">Yi Yu (University of Warwick)</a>
            </td>
        </tr>
        <tr>
            <td class="speaker">
             Private distributed functional data analysis
            </td>
        </tr>
            <td class="date" rowspan="2">

            </td>
        <tr>
            <td class="abstract">
              Abstract: In this talk, we study private distributed functional data analysis problems, for both mean estimation and varying coefficient model estimation.  We show the interplay of federated, central and user-level differential privacy notions.  Our results show a number of interesting phase transition phenomena, from sparse to dense and from non-private to various private notions.
            </td>
        </tr>
    </table>

    </p>



<footer>
    &copy; Conference Organizers
    &nbsp;|&nbsp; Design by <a href="https://github.com/mikepierce">Mike Pierce</a>
</footer>

</body>
</html>
