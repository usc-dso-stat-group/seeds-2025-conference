<!DOCTYPE html>
<html lang='en'>

<head>
    <base href="..">
    <link rel="shortcut icon" type="image/png" href="assets/favicon.png"/>
    <link rel="stylesheet" type="text/css" media="all" href="assets/main.css"/>
    <meta name="description" content="SEEDS Conference">
    <meta name="resource-type" content="document">
    <meta name="distribution" content="global">
    <meta name="KeyWords" content="Conference">
    <title>Sessions | SEEDS Conference</title>
</head>

<body>

    <div class="banner">
        <img src="assets/banner.jpg" alt="SEEDS Conference">
        <div class="top-left">
            <span class="title1">SEEDS</span><span class="title2">Conference</span> <span class="year">2025</span>
        </div>
        <div class="bottom-right">
            January 8-11, 2025 <br> University of Southern California, Los Angeles (CA)
        </div>
    </div>

    <table class="navigation">
        <tr>
            <td class="navigation">
                <a title="Conference Home Page" href=".">Home</a>
            </td>
            <td class="navigation">
                <a title="Register for the Conference" href="registration">Registration</a>
            </td>
            <td class="navigation">
                <a class="current" title="Sessions" href="sessions">Sessions</a>
            </td>
            <td class="navigation">
                <a title="Conference Schedule" href="schedule">Schedule</a>
            </td>
            <td class="navigation">
                <a title="Conference venue" href="venue">Venue</a>
            </td>
        </tr>
    </table>


    <br>

    <a name=“keynote_presentations”>
    <h3>Keynote Presentations</h4>
    </a>

    <table>
        <tr>
            <td class="date" rowspan="2">
             Speaker 1
            </td>
            <td class="title">
                <a href="https://fan.princeton.edu/">Jianqing Fan (Princeton University)</a>
            </td>
        </tr>
        <tr>
            <td class="speaker">
             Spectral Ranking Inferences Based on General Multiway Comparisons
            </td>
        </tr>
            <td class="date" rowspan="2">

            </td>
        <tr>
            <td class="abstract">
              Abstract: This paper studies the performance of the spectral method in the estimation and uncertainty quantification of the unobserved preference scores of compared entities in a general and more realistic setup. Specifically, the comparison graph consists of hyper-edges of possible heterogeneous sizes, and the number of comparisons can be as low as one for a given hyper-edge. Such a setting is pervasive in real applications, circumventing the need to specify the graph randomness and the restrictive homogeneous sampling assumption imposed in the commonly used Bradley-Terry-Luce (BTL) or Plackett-Luce (PL) models. Furthermore, in scenarios where the BTL or PL models are appropriate, we unravel the relationship between the spectral estimator and the Maximum Likelihood Estimator (MLE). We discover that a two-step spectral method, where we apply the optimal weighting estimated from the equal weighting vanilla spectral method, can achieve the same asymptotic efficiency as the MLE. Given the asymptotic distributions of the estimated preference scores, we also introduce a comprehensive framework to carry out both one-sample and two-sample ranking inferences, applicable to both fixed and random graph settings. It is noteworthy that this is the first time effective two-sample rank testing methods have been proposed. Finally, we substantiate our findings via comprehensive numerical simulations and subsequently apply our developed methodologies to perform statistical inferences for statistical journals and movie rankings. (Joint work with Zhipeng Lou,  Weichen Wang, and Mengxin Yu)
            </td>
        </tr>
    </table>

    <table>
        <tr>
            <td class="date" rowspan="2">
                Speaker 2
            </td>
            <td class="title">
                <a href="https://www.statslab.cam.ac.uk/~rjs57/">Richard J. Samworth (University of Cambridge)</a>
            </td>
        </tr>
        <tr>
            <td class="speaker">
            How should we do linear regression?
            </td>
        </tr>
            <td class="date" rowspan="2">

            </td>
        <tr>
            <td class="abstract">
                Abstract: In the context of linear regression, we construct a data-driven convex loss function with respect to which empirical risk minimisation yields optimal asymptotic variance in the downstream estimation of the regression coefficients. Our semiparametric approach targets the best decreasing approximation of the derivative of the log-density of the noise distribution. At the population level, this fitting process is a nonparametric extension of score matching, corresponding to a log-concave projection of the noise distribution with respect to the Fisher divergence. The procedure is computationally efficient, and we prove that our procedure attains the minimal asymptotic covariance among all convex M-estimators. As an example of a non-log-concave setting, for Cauchy errors, the optimal convex loss function is Huber-like, and our procedure yields an asymptotic efficiency greater than 0.87 relative to the oracle maximum likelihood estimator of the regression coefficients that uses knowledge of this error distribution; in this sense, we obtain robustness without sacrificing much efficiency. 
            </td>
        </tr>
    </table>

    <table>
        <tr>
            <td class="date" rowspan="2">
                Speaker 3
            </td>
            <td class="title">
                <a href="https://www.hsph.harvard.edu/profile/xihong-lin/">Xihong Lin (Harvard University)</a>
            </td>
        </tr>
        <tr>
            <td class="speaker">
              Navigate the Crossroad of Statistics, ML/AI and Domain Science
            </td>
        </tr>
            <td class="date" rowspan="2">

            </td>
        <tr>
            <td class="abstract">
Abstract: The data science ecosystem encompasses data fairness, scalable statistical and ML/AI methods and tools, interpretable data analysis, and trustworthy decision-making. Rapid advancements in AI have revolutionized data utilization and enabled machines to learn from data more effectively. Statistics, as the science of learning from data while accounting for uncertainty, plays a pivotal role in addressing complex real-world problems and facilitating trustworthy decision-making. In this talk, I will discuss the challenges and opportunities as we navigate the crossroad of statistics and AI, including how to build an end-to-end scalable data science ecosystem, leverage AI/ML-prediction to empower statistical analysis of biobank data,  transfer inference for genetic association analysis,  build the whole genome variant functional annotation database and portal FAVOR and FAVOR-GPT,  and incorporate multi-faceted variant functional annotation to boost power of whole genome sequencing end-to-end analysis. This talk aims to ignite proactive and thought-provoking discussions, foster cross-disciplinary collaboration, and cultivate open-minded approaches to advance scientific discovery.
            </td>
        </tr>
    </table>

    <a name=“short_courses”>
    <h3>Short Courses</h4>
    </a>

    <table>
        <tr>
            <td class="date" rowspan="2">
             Short Course 1 (January 8, 9 am-12 pm)
            </td>
            <td class="title">
                <a href="https://jjcherian.github.io/">John Cherian (Stanford University)</a>
            </td>
        </tr>
        <tr>
            <td class="speaker">
             Title: Making sense of the black box: what can conformal inference offer the theory and practice of data science?
            </td>
        </tr>
            <td class="date" rowspan="2">
            </td>
        <tr>
            <td class="abstract">
              Abstract: TBA
            </td>
        </tr>
    </table>

    <table>
        <tr>
            <td class="date" rowspan="2">
               Short Course 2 (January 8, 1:30 pm-4:30 pm)
            </td>
            <td class="title">
                <a href="https://wesgruver.ai/">Wes Gruver (Databricks)</a>
            </td>
        </tr>
        <tr>
            <td class="speaker">
            TBA
            </td>
        </tr>
            <td class="date" rowspan="2">

            </td>
        <tr>
            <td class="abstract">
                Abstract: TBA 
            </td>
        </tr>
    </table>

    <table>
        <tr>
            <td class="date" rowspan="2">
                Short Course 3 (January 10, 2 pm-5 pm)
            </td>
            <td class="title">
                <a href="https://www.linkedin.com/in/haoda-fu-17a5256/">Haoda Fu (Amgen)</a>
            </td>
        </tr>
        <tr>
            <td class="speaker">
              Tutorial on Deep Learning and Generative AI
            </td>
        </tr>
            <td class="date" rowspan="2">

            </td>
        <tr>
            <td class="abstract">
Abstract: This course is tailored for individuals with a solid background in statistics or biostatistics, focusing on deep learning and generative AI. Starting with foundational deep learning concepts, participants will implement models using PyTorch, explore popular AI architectures like CNN, GNN, ResNet, U-net, and transformers, and address applications in medical imaging and drug discovery. The course will also introduce generative AI including GANs, VAEs, DDPM, score-based models, and also how LLM works. It offers a comprehensive insight by providing mental models into applying AI in healthcare, research, and beyond.
            </td>
        </tr>
    </table>

    <h2>Invited Sessions</h2>
    <p>
      Details TBA.
    </p>



<footer>
    &copy; Conference Organizers
    &nbsp;|&nbsp; Design by <a href="https://github.com/mikepierce">Mike Pierce</a>
</footer>

</body>
</html>
