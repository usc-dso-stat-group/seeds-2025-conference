Differentially Private Quantile Regression with Applications to Inventory Policy Learning 

We introduce a novel approach for privacy-preserving quantile regression within the f -differential privacy framework, an extension of the classical (ϵ, δ)-differential privacy with several appealing properties. A key challenge is the nonsmoothness of the quantile loss function, which sets it apart from existing work on privacy-preserving algorithms in other settings. We develop a clipped noisy gradient descent algorithm based on convolution smoothing for quantile regression, and show that the algorithm provides privacy guarantees and desirable statistical precision. We derive finite-sample high-probability bounds for parameter estimation and regret analysis. Our bound aligns with that for strongly convex and smooth loss function. We apply the approach to the data-driven newsvendor problem with features and show that we attain a faster excess population risk bound compared to that obtained from an indiscriminate application of existing results for general nonsmooth convex loss. Our numerical experiments demonstrate that the proposed new method can achieve desirable privacy protection with a marginal increase in cost. (Joint work with Tuoyi Zhao and Wenxin Zhou).
