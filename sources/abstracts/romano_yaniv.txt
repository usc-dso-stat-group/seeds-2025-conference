Robust Conformal Prediction Using Privileged Information 

This talk introduces a novel method for generating uncertainty sets for ML predictions with guaranteed coverage for the unknown test label in the face of distribution shifts induced by corrupted training data, such as missing or noisy variables. Our approach builds on conformal prediction—a powerful framework for constructing uncertainty sets—but addresses its limitations in scenarios involving such distribution shifts. Central to our method is the use of privileged information (PI)—additional features available only during training that capture valuable insights about the training data, such as the expertise level of annotators, time spent on labeling, or disagreements among annotators. While such PI features are unavailable at test time, they play a crucial role in explaining the distribution shift, paving the way to construct valid and robust uncertainty estimates. I will demonstrate the practical utility of this method through real-world applications, including scenarios with missing labels and, if time permits, unobserved counterfactuals at test time in causal inference tasks.
