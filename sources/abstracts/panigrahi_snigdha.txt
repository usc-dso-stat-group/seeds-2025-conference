Cross-validation with antithetic Gaussian randomization 

In this talk, I will introduce a new method for performing cross-validation using “antithetic” Gaussian randomization variables. The randomization variables in our method are drawn from an equicorrelated, degenerate normal distribution. Each pair of these randomization variables is maximally negatively correlated, which is why we describe this randomization scheme as “antithetic”. Inspired by recent data-splitting techniques such as data-fission and data-thinning, the new cross-validation method is well-suited for problems where traditional sample splitting is infeasible. Even in scenarios where traditional sample splitting is possible, our cross-validation method offers a computationally efficient alternative for estimating prediction error. By reducing the amount of randomization in the train data, our method achieves bias levels as small as the standard leave-one-out cross-validation, while requiring only a small number of train-test repetitions---potentially as few as two. A key advantage of our cross-validated estimator is its stable variance, which does not increase even as the bias from estimating the prediction function on the training data approaches zero. In both theory and simulations, we show that this desirable bias-variance property of our cross-validated estimator extends to a wide range of loss functions, including those commonly used in generalized linear models. This is based on joint work with Sifan Liu and Jake Soloff.
